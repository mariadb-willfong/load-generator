Realistic Global Banking Data Generation & Simulation Plan

Banking Data Schema and Entities for a Global Bank

Figure: Entity-relationship model of a basic online banking system, illustrating Customers, Accounts, Transactions, and related entities  .

To simulate a “bank in a box,” we first define a comprehensive schema that reflects a real core banking system. As in most core banking designs, we have canonical entities including Customer, Account, Transaction (and possibly Product types and Ledger entries) . Key tables and their attributes would include:
	•	Customer – Stores customer PII details such as name, date of birth, contact info, and address. Each customer is assigned a home country/branch and a time zone to model global distribution. For realism, customer names and addresses should be generated to reflect worldwide diversity (e.g. various ethnic names, but using a single language encoding like English). Each customer also has authentication data (e.g. username and hashed password) for online banking login . Customers are marked with attributes like segment or wealth tier (regular, premium, corporate, etc.) to drive behavior differences.
	•	Account – Represents a bank account owned by a customer. An account record links to its owner via a foreign key (CustomerID) . Important fields include Account Type (e.g. checking/current, savings, credit card, loan) , Currency (for a global bank, accounts may be in different currencies), Balance (with proper numeric precision for money), and status (active, closed, etc.). Each customer can have multiple accounts (one-to-many relationship) . Account numbering should mimic real conventions (branch codes, etc., if needed). We will include both retail accounts (individuals) and business accounts (companies) – the latter are crucial since retail customers will interact with business accounts (for example, paying a company or receiving payroll). Business entities can be distinguished either by a flag in the Customer table or a separate Company table; at minimum we generate a variety of corporate accounts to serve as transaction counterparties (merchants, employers, utilities, etc.).
	•	Transaction – Records every movement of money into or out of accounts, capturing the ledger of customer activity. Each transaction will have an auto-generated ID, a timestamp, the involved account(s), amount (with a sign or type to indicate credit/debit), and a type code (e.g. deposit, withdrawal, purchase, transfer) . In our schema, a single transaction may involve one primary account (e.g. a cash withdrawal from one account) or two accounts (e.g. a transfer from account A to B). We can model this by logging two entries (one debit on A, one credit on B) or by a single record that references both a source and destination account. A straightforward approach is to record each account’s ledger movement as one row (similar to double-entry bookkeeping in the database): for an internal transfer between two accounts in the bank, create two transaction records (one debit, one credit) linked by a common transaction ID or reference. This way, the Transaction table essentially acts as an account ledger table. Each transaction row would include fields like: AccountID, amount, balance update, counterparty info (e.g. the other account or an external beneficiary), channel (ATM/online/branch), and a textual memo/description. Including a realistic text memo (like “ATM withdrawal at New York Branch 005” or “Payment to ACME Utilities Inc.”) ensures each row’s size is similar to production (but avoid random gibberish just to bloat size, as that would unrealistically skew performance metrics). The text should resemble real descriptions a bank would store, not nonsense filler.
	•	Beneficiary/Payee (External Parties) – To simulate payments leaving the bank, we include a concept of external payees. This could be a Beneficiary table storing payees that customers can transfer money to, including fields like name, external bank info, account number, etc. . For simplicity, many “external” transfers might actually go to business accounts within our bank (so that all transactions remain in our data), but we can still log some transactions as going to external accounts via beneficiaries. This adds realism for actions like “Add Payee/Beneficiary” and “Bill Payment” in online banking.
	•	Branch/ATM – Since it’s a global bank, we can have a reference table for branches and ATMs (each with an ID, location, time zone). This lets us tag transactions with a branch or ATM ID when relevant. For example, an ATM withdrawal transaction record could include the ATM ID or branch ID where it occurred. This is useful for realistic log generation and for potential hotspot modeling (e.g. some ATMs or branches see more traffic).
	•	Audit Log – In addition to financial transactions, real banks maintain extensive audit trails of user actions. We will include a logging mechanism (which could be a separate table or logged to a file) that captures non-monetary events and metadata: e.g. user login attempts, authentication failures, changes to account details, and any transaction initiation events. Each log entry should record who performed the action (customer ID or employee/admin ID), what action was attempted, when (timestamp), where (channel or location – e.g. IP address for online, ATM ID for ATM use), and the outcome (success/failure) . For instance, a failed login would generate an audit log entry with the user ID, timestamp, source IP, and reason (failed password). Every successful transaction would similarly have an audit entry referencing the transaction ID and indicating which channel initiated it. These audit logs provide the “who, what, when, where” for each event, which is crucial for security and compliance . We will ensure that every customer interaction in the simulation (login, transaction, etc.) results in an audit trail record. This mimics real core systems where actions are chronologically logged for accountability and fraud detection.

This schema covers the bread-and-butter of a core banking system: customers, accounts, transactions, and logs . It’s important to enforce foreign keys (e.g. transactions reference valid accounts, accounts reference valid customers) to maintain referential integrity, just as a real bank would. All monetary values will use fixed-point numeric types for accuracy . We will also create appropriate indexes (e.g. on transaction account ID and date, on customer login username, etc.) to support query patterns – but we will consider when to create these indexes as discussed later.

Common Transactions, Queries, and Access Patterns

To stress the database realistically, we simulate the typical mix of banking operations across different channels. The workload should reflect that banks handle a lot of read queries (inquiries) relative to writes, since customers frequently check information in addition to making transactions. We will incorporate a balanced read/write mix but with variation per user and session, as real usage isn’t uniform. Some key operations and queries include:
	•	Balance Inquiries (Read): One of the most common actions. Customers check their account balance via ATM or online. This translates to a simple query on the Account table (or a fast lookup of a cached balance). These are read-only queries, and in realistic usage they occur very frequently – many customers check their balances daily or weekly. (In fact, over a third of users check their accounts daily while some only do so monthly  , indicating a wide range of inquiry frequency). We will simulate this by having a large volume of balance-check events in the workload.
	•	Transaction History View (Read): Customers often review recent transactions (e.g. last 10 transactions or last 30 days of activity). This results in a range query on the Transactions table filtering by AccountID and date, often sorted by date. We will include such queries, which are heavier than single-balance lookups because they scan multiple rows. These will be common in online banking sessions (e.g. a user logs in and scrolls through their statement). We should optimize by having an index on (AccountID, Timestamp) for the Transactions table so these range queries are efficient.
	•	Funds Transfer (Write+Read): Transferring money from one account to another (either internal or external). This is a multi-step operation: the user may first do a payee lookup or selection (read from Beneficiary list), possibly check current balance (another read), and then the actual transfer transaction which debits one account and credits another (writes). We will simulate internal transfers (between accounts within the bank) as well as external payments (from a customer account to an outside beneficiary, which for our DB might appear as a debit with an external counterparty note). Each transfer will produce two transaction records (debit and credit) if both source and destination are within the bank, or one transaction record (debit) plus perhaps an audit entry of an outgoing payment if the destination is external. This operation updates balances on the accounts involved. We will ensure to enforce business rules (no overdrafts unless an overdraft account, etc.) so that some transfer attempts can fail due to insufficient funds (see Error Simulation below).
	•	Cash Withdrawal (Write+Read): When a customer withdraws cash from an ATM or teller, it’s a debit transaction on their account. Typically, the ATM session will also do a balance inquiry before or after dispensing cash. So an ATM session often involves one read (balance) followed by one write (debit transaction) and a balance update. We will model ATM withdrawals accordingly. The transaction record will note it was via ATM (channel and ATM ID), include perhaps a fee if applicable, and the audit log will capture the ATM’s identification and that cash was dispensed.
	•	Deposits (Write+Read): Less frequent than withdrawals in many contexts (especially as direct deposit is common), but we can simulate cash or check deposits either via ATM or branch. This would credit the account. For simplicity, deposit events can be generated to certain accounts (or we simulate paychecks as described later under patterns). A deposit might be preceded by an account lookup and followed by a new balance inquiry.
	•	Login/Authentication (Read): Each online banking session starts with a login. This involves reading the user’s credentials (and possibly updating last login timestamp or logging an event). We will simulate logins as separate events generating audit logs. Though not heavy on the DB (a small user lookup), it’s important to log successes and failures for realism and security.
	•	Other Account Maintenance (Mixed): For completeness, we might simulate occasional updates like changing an address or password (writes to Customer table), opening or closing accounts (inserting or updating Account records), adding a new beneficiary (insert into Beneficiary table), etc. These are less frequent but add background load and more writes to non-transaction tables.

Typical query patterns and load mix: In an online banking context, read operations dominate in count. A study of consumer habits shows many users check balances daily but only initiate transactions (payments, transfers) a few times a week or month . We will reflect this by having a ratio such as several inquiries per each money movement. For example, a single user session might include 3–5 balance or history views around 1 payment. ATMs might have a roughly 1:1 ratio (most ATM users check balance then withdraw cash). Overall, the system might see perhaps ~5 reads per write on average (this is an estimate; the exact mix will be randomized per user as specified). We will randomize the read/write mix for each simulated customer session – some sessions may just browse without making any transaction, while others perform one or multiple transfers. By varying this, we ensure the workload covers both read-heavy scenarios and write-heavy scenarios.

The database indexes and queries will be tuned to these patterns. For instance, indexing account balances and primary keys will make single-record lookups fast, and indexing transactions by account and date will support history queries. We’ll also prepare typical SQL queries for each operation (e.g. SELECT balance FROM Account WHERE AccountID = ? for balance check, or a join between Customer and Account for certain reports) and ensure they follow best practices (using parameterized queries, proper filtering, etc.). Complex analytical queries are less common in the live workload (those might be offloaded to a data warehouse in real banks), so our focus is on OLTP-style queries: simple and transaction-focused.

Logging, Audit Trails, and Error Handling

Realism requires that we mix in failure and error scenarios rather than assuming every action succeeds. Banks track every attempt in detail, so our simulation will generate realistic audit logs and error cases:
	•	Audit Trail Content: As noted, every action creates a log. The audit log table (or file) will have columns like Timestamp, CustomerID, ActionType, Target (which account or entity was affected), Channel (ATM, Online, etc.), Location (which branch/ATM or IP), and Outcome. This aligns with best practices where an audit trail records source of transaction, destination, date/time, details, and user ID . For example, a funds transfer audit entry might say: “2025-12-01T10:05:00Z – User 12345 – TRANSFER – From Account 111111 to Account 222222 – Online Banking – Success”. A failed ATM withdrawal might log as: “2025-12-01T10:06:00Z – User 12345 – WITHDRAWAL – Account 111111 at ATM #987 – Failed (Insufficient Funds)”. These logs are invaluable for later analysis, debugging, or compliance verification.
	•	Failed Logins: We will simulate scenarios such as customers entering wrong password/PIN. A proportion of login attempts (say a few percent) will be failures. These should not create a transaction, but do create an audit log (with outcome “failed – incorrect password”). If multiple failures occur, perhaps simulate lockouts or escalation (though that might be beyond our scope, simply logging is fine).
	•	Insufficient Funds / Declined Transactions: Not all attempted payments or withdrawals will succeed. We’ll introduce random cases where a customer tries to spend more than their balance or above daily limits. The system should detect it and decline the transaction. In simulation, we log an audit event for the attempt (with outcome “failed – insufficient funds”) and we may either (a) not insert a transaction record at all for a declined operation, or (b) insert a record in a separate “FailedTransactions” table for analysis. Real banks typically would not record an entry in the account ledger for a failed attempt (since no money moved), but they might keep the request in an internal log. For our purposes, it’s sufficient to capture it in the audit log and proceed.
	•	Other Errors: We can include sporadic technical failures – e.g. a network timeout or an ATM out-of-service mid-transaction. In such cases, the transaction might or might not go through. We could simulate a small percentage of transactions that get retried due to timeouts (ensuring not to double-post in the database by, say, using an idempotency key). Also, perhaps simulate occasional data entry errors (transferring to a non-existent account, etc.) resulting in immediate validation errors. These scenarios add noise to the workload and test the system’s robustness. All of these should be reflected in logs with appropriate error codes/messages.

By including errors and exceptions, we “mix in everything” as requested, which mirrors production where not every interaction is a smooth path. This will exercise database constraints (e.g. foreign keys will reject invalid account targets) and will generate a richer dataset (with audit logs showing both successful and failed operations).

Phase 1: Bulk Generation of Historical Data

Phase 1 is about seeding the database with a realistic historical backlog of data before the live simulation begins. In a real bank, this would be years of transactions accumulated; for our testing purposes, we’ll generate it in bulk. Key recommendations for Phase 1 data generation:
	•	Full Schema Coverage: We will simulate all aspects of the schema (as “Simulate all of it” suggests). That means generating customers, accounts for each, and transactions covering various operations in the past. We assume sufficient CPU/resources to generate a large dataset.
	•	Time Span: Determine how far back the history goes (e.g. 5 years of data, or more if needed). The generation should iterate through days and create realistic transaction volumes per day per account. We can incorporate historical patterns as well – for example, ensure that end-of-month historically shows spikes due to payroll (so every month’s end in the synthetic history has a surge of transactions) and perhaps growth in customer numbers over time (if simulating a growing user base).
	•	Volume and Distribution: Use knowledge of banking volumes to decide how many transactions to create. For instance, if we have N customers, each might have on average M transactions per month. This could be scaled to reach a target size (like millions of rows). The distribution should be skewed: a small percentage of very active accounts might have hundreds of transactions per month (e.g. a business or a high-net-worth individual), whereas some accounts might only see a few transactions (salary in, a few bill payments, cash withdrawals). Adopting a Pareto-like distribution for activity can reflect reality, where, for example, 20% of accounts might account for 80% of transaction volume. This ensures hotspots and avoids an overly uniform dataset.
	•	Data Realism: Populate the data with realistic values. For PII, use real-world name lists by country to assign customer names consistent with their country (but all text stored in English/ASCII to avoid complicating encoding). Addresses can be faked but plausible (city names, postal codes that match countries, etc.). Transaction amounts should follow realistic ranges: e.g. many small purchases (a few dollars), daily ATM withdrawals often e.g. $20-$200, paychecks and rent payments in the higher range, etc. We can randomize amounts within logical bounds for each transaction type (perhaps using statistical distributions or fixed ranges per type). Dates and timestamps of transactions should be assigned in a chronologically sensible way (no future dates, etc.), and ideally preserving some correlation (salary deposits on last weekday of month, utility bills paid at start of month, spikes on Black Friday for shopping if US, etc.). All this will make the data feel authentic when we analyze it.
	•	Concurrent File Generation: To speed up the initial load, generate one bulk data file (e.g. CSV) per table (customer, account, transaction, etc.). This segmentation (one file per table) allows parallel loading into the database, utilizing multiple threads or bulk loader processes at once. For example, we can simultaneously load the Customers file and Accounts file and then Transactions, since accounts depend on customers and transactions depend on accounts (we’d ensure referential integrity by generating in that order). The disk subsystem will likely be the bottleneck when inserting millions of rows, but concurrent load can maximize throughput (point 15 was to do this).
	•	Index Creation Strategy: During the bulk load (Phase 1), it’s typically faster to load data into tables without secondary indexes, and create the indexes afterward in a single pass . We recommend deferring index and constraint creation until after the data is loaded (for Phase 1 only). This is not how a live system operates, but it greatly improves bulk loading speed. So Phase 1 will: (a) create tables with only primary keys (and perhaps disable foreign key checks initially), (b) load all data, then (c) add the necessary indexes and enable foreign key constraints, letting the DB validate the consistency in batch. This matches the plan: Phase 1 indexes should be created after data load, since Phase 1’s goal is efficient seeding rather than mimicking operational realism. After Phase 1, the database will be brought to a state as if it had organically grown that data.
	•	Foreign Keys and Integrity: We will ensure that foreign keys (Account->Customer, Transaction->Account, etc.) are satisfied by our generated data. If foreign key enforcement was off during load, we turn it on afterward, trusting our generator to have not violated any (if it did, the DB will error and we’d need to fix the data gen accordingly). For example, every transaction’s AccountID must exist in the Account table. We will not generate any orphan records. Also, if we want to simulate historical account closures or customer churn, we might generate some accounts with a closed status (and no transactions after closure date, etc.) – but any such historical edge cases will be consistent (e.g. no transactions on a closed account beyond its closure date).
	•	PII and Privacy: Since this is synthetic, we include realistic PII data but ensure it’s fake. One must be careful not to accidentally use any real customer data. Using libraries or open datasets (like lists of common names by country, etc.) can help. Also, because we only need English support, we won’t simulate multilingual data (all data will be in English or basic Latin script, even for customers from non-English-speaking countries, to keep things simple).
	•	No Need for Complex Warmup: We won’t simulate a “system warmup” in Phase 1 – we are effectively pre-warming by loading historical data. Once Phase 1 is done, the database is populated and can be considered “hot” with a sizable dataset. There’s no separate warmup run needed (point 20: no explicit warmup phase; we’ll go straight into Phase 2).

After Phase 1, we will have a fully loaded database representing a global bank’s state up to now: millions of transactions, thousands or millions of customers across many countries, each with accounts and balances that reflect a long history of activity. Indexes will be in place to support the upcoming live simulation, and the system will have logs and records akin to a real core system.

Phase 2: Live Customer Interaction Simulation

Phase 2 involves driving an active workload against the database in real time, as if millions of customers were concurrently using the bank’s services. The goal is to be “as close to real life as possible.” We will create a Go program (one binary) that simulates X number of concurrent customers (configurable) performing actions with realistic timing and behavior. Key aspects of this live simulation:

Concurrency Model and Sessions

We will simulate concurrent user sessions using goroutines (or threads). For example, if we want to simulate 10,000 active users, the program can spawn 10,000 goroutines, each representing one customer interacting with the bank’s systems. Each simulated customer can loop through a realistic sequence of actions (with pauses in between). The concurrency level X (number of parallel users) should be configurable to scale the load.

Each session will mimic a realistic workflow. For instance, an ATM session might be: user arrives at ATM -> insert card (authenticate/PIN) -> check balance -> withdraw cash -> take receipt and leave. An online banking session might be longer: user logs in -> views balances of all accounts -> opens a transactions history page for last 2 months -> maybe downloads a statement (could be another read query) -> initiates a funds transfer or pays a bill -> logs out. We will design a set of such workflows and assign them to sessions probabilistically (e.g. 30% of sessions are ATM-type quick sessions, 50% are regular online sessions with maybe one transfer, 20% are just login and check info with no transaction). We’ll also incorporate business account activity sessions (for corporate users or internal processes): e.g. a company uploads a payroll batch at end of month, or an e-commerce merchant account receiving many small payments throughout the day.

Think time and pacing: Each simulated user will wait (sleep) for realistic intervals between actions, to simulate the human or network delays. For example, after logging in, the user might spend 5-10 seconds before the next action (reading the screen), after initiating a transfer they might pause a couple seconds, etc. These think times can be randomized within reasonable ranges. We do this to avoid an unrealistic firehose of back-to-back queries with no delay. The result will be a more bursty but human-like pattern of database requests.

The simulation will be designed to run indefinitely (or until a configured duration) – there is no fixed stopping condition by default (point 19: we will manually stop it when needed). This allows long-running tests (e.g. soak test for many hours) until manually terminated.

Time Zones and Working Hours

Because this is a global bank, we will assign each customer a home time zone (based on their country or branch). This is critical to simulate realistic access times: customers typically use the bank during daytime hours (roughly 8 AM to 4 PM local time), not uniformly 24/7. Our simulation will therefore schedule each customer’s session activity mostly during their local daytime. Concretely, if a user is assigned to GMT+5, their session loop will mostly perform actions between 08:00 and 16:00 GMT+5 (with some randomness) and be mostly idle outside that window. A simple way to implement this is to give each session a probability of being active at a given minute of the day based on the time of day and their profile. We might park the session (no operations) during their night hours, or drastically reduce their activity frequency at night.

By doing this for customers across all time zones, the aggregate system load will follow a “wave” pattern following the sun – there is always some region in business hours. We won’t see the entire system go idle at once (since when Asia is sleeping, Europe or America is active, etc.). However, we will see regional peaks: e.g. when it’s 8-9 AM in each region, many people log in to check accounts in the morning; around noon, maybe a slight ATM spike for lunch cash; in the evening some people do final transactions, etc. The result is a realistic cyclical load that ebbs and flows throughout a 24-hour cycle, rather than a constant rate. This adheres to the instruction to simulate usage at local 8am-4pm times.

Daily and Weekly Patterns

We incorporate known daily/weekly cycles of banking operations:
	•	Weekday vs Weekend: Generally, transaction volumes are higher on weekdays (when businesses are open, payrolls processed, markets active) and somewhat lower on weekends. We will simulate fewer corporate transactions on weekends (since businesses and payroll offices are closed), but retail customers might still use ATMs and online banking on weekends (especially Saturdays). Sundays might be quietest in some cultures. We’ll adjust the per-user activity probabilities accordingly (e.g. a person might log in daily on weekdays but rarely on Sunday).
	•	End-of-Month Payroll Spikes: A crucial pattern is the payroll spike at month-end. In many countries, salaries are paid on the last day or last Friday of the month. This causes a surge of incoming transactions (credits to many employees) and subsequent outgoing transactions (people paying bills or increased spending after getting paid). To model this, we will include a special event on, say, the 25th-30th of each month: corporate payroll accounts will initiate large numbers of salary transfers (one company crediting thousands of employees’ accounts). This will create a high volume burst of transactions (writes) on those days, and a ripple of increased balance inquiries and spending afterward. Industry data confirms that scheduled events like salary day can trigger transaction surges multiple times higher than normal load . Our simulation will explicitly generate these end-of-month batch payments (maybe concentrated around 2 PM local time of that country’s payroll processing). We’ll see peak TPS during these periods, which is exactly what happens in real systems (banks plan capacity for payroll peaks and similar events).
	•	Other Periodic Peaks: We can also simulate monthly bill payment cycles (e.g. many bills are due at the 1st of the month, so end of month and beginning of month might see spikes in bill pay transactions). Additionally, if the bank operates in regions with known shopping festivals or holidays (Singles’ Day in China, Black Friday, Diwali, etc.), those could cause spikes in card transactions. While we may not simulate specific holidays, we can incorporate a generic concept of occasional high-volume days outside the regular schedule to test burst handling.
	•	Intraday Pattern: Within a single day (particularly a weekday), bank transaction traffic isn’t flat. We might see a morning peak (people checking accounts after overnight batch processes or salary deposits), a midday peak (ATM withdrawals at lunch, or online banking during lunch breaks), and a smaller peak toward end of business day (people finishing transfers before cut-off times). We will program the simulation to weight operations accordingly by time of day. For example, around 9:00 AM local time, a lot of balance inquiries and some transfers (as folks start their day); around 12:00, more ATM cash withdrawals; 4:00 PM might have a spike of last-minute fund transfers (since some banks have a 5 PM cutoff for same-day transfers). After hours (late evening), traffic drops significantly, except perhaps card transactions which happen 24/7 but those might not be in scope unless we simulate card swipes as part of transactions.

By capturing these daily and weekly rhythms, the workload will have periods of high and low intensity just like production. We can measure performance across these cycles.

Burst and Peak Load Modeling

In addition to predictable cycles, we want to incorporate random bursts to stress the system. Burst modeling means occasionally the system will experience a short-term spike in activity that wasn’t strictly scheduled – for example, a viral event or an outage recovery causing a sudden backlog flush. Industry standard benchmarks often include bursty patterns to test elasticity . We will implement bursts by temporarily increasing the activity rate of sessions or by spawning additional load threads for a short duration:
	•	Lunch-time ATM burst: e.g. every day at 12:00 local time in each region, we could double the ATM withdrawal rate for about 15 minutes to simulate many people queuing at ATMs.
	•	Unplanned spikes: We might randomly choose a few times per week to surge the overall load (all users’ think times are halved for 10 minutes, for instance) to mimic unpredictable peak events.
	•	Promotional events: If we simulate something like a flash sale or tax deadline, it could cause many transactions in a short window. For example, at 10:00 AM, simulate that a popular investment product became available and many people transfer money to their investment accounts, spiking transfer volume briefly.

Our approach to bursts will use “industry standard” values or models where available. One simple model is using a Poisson process for transactions but with time-varying rate (basically what we described with peaks). Another idea is to have background batch jobs (like overnight interest calculation posting interest to millions of accounts at midnight in certain time zones – that could spike writes as well if we include interest transactions monthly or quarterly).

Importantly, the system should be designed to handle these bursts. We ensure that any built-in think time or pacing can be overridden for a burst. Monitoring the database under these conditions will tell us how it behaves under stress.

Customer Segmentation and Behavior Variability

Not all customers behave the same – in fact, realistic workload must reflect different wealth and activity tiers among the user base (point 12):
	•	Wealth Tiers: We will categorize customers (even if implicitly) into tiers such as low-balance customers, middle-class, and high-net-worth individuals (or perhaps retail vs private banking). Lower-tier customers might have lower balances and smaller transaction amounts (their salary might be modest, they withdraw most of it in cash, etc.), whereas high-tier customers have large balances, maybe multiple accounts (savings, investment, etc.), and occasionally move very large amounts (buying a car, large wire transfers). This affects transaction frequency and sizes. For instance, a wealthy client might not hit the ATM often but could have a few big transfers (paying off a credit card with $10k, or transferring $50k to buy stocks) – those events are rare but significant. Meanwhile, a student customer might have many small debit card transactions (coffee, groceries) if we were simulating card swipes, but if not, at least ATM withdrawals of small amounts and a lot of balance checks to avoid overdraft. We will randomize each customer’s assigned tier and then drive their simulation behavior accordingly (so the load generator knows, for example, 10% of users are high-net-worth and will generate different pattern vs 50% who are normal salary earners).
	•	Activity Levels: Some customers are simply more active than others. We can model this by giving each customer an “activity score” that influences how often they log in or transact. This can be correlated with wealth (e.g. businesses and high-wealth might generate more absolute transactions, but even among low-wealth there are differences in how people use the account). We ensure a skewed distribution: e.g. a minority of users might generate a majority of transactions. This follows real banking observations where a few accounts (like corporate payroll accounts or merchants) see extremely high traffic, whereas the median customer might only have a handful of transactions per month. By introducing this skew, we also create hotspot accounts and hotspot branches – e.g. one branch might have a very active ATM serving a city center, contributing lots of transactions, or one corporate account does thousands of payroll transfers. This is desirable for testing performance on hot rows, index contention, etc.
	•	Geographic and Cultural Variation: Because we have customers in every country, their behavior might vary. For example, in some countries cash usage is still high (so more ATM withdrawals per person), while in others digital payments dominate (customers log in to transfer or use mobile wallets frequently). We can incorporate a regional factor: e.g. flag some countries as “cash-heavy” and have those customers use ATMs more (multiple times a week), versus “cash-light” regions where ATM usage is rare and online transfers are more common. Another aspect is time-of-week: in predominantly Muslim countries, Friday might be low activity (holiday), whereas Sunday is a normal business day in others – we can tweak weekly patterns per region if needed. These nuances add realism, although even a simpler global model will suffice if too granular.
	•	Business vs Retail Behavior: We generated business accounts – their usage is different. A business account (say a store) might receive many small deposits (customers paying via POS, which could be simulated as batch or individual deposits into their account) and then do a few large withdrawals or transfers (sweeping money to a corporate treasury account, or paying suppliers). We will simulate a handful of archetypes: e.g. “Payroll account” (lots of outgoing at month end), “Merchant account” (lots of incoming daily), “Utility company” (many incoming bill payments around start of month), “Small business” (moderate daily activity both directions). These will act as counterparties to individual transactions (e.g. when a retail customer pays their phone bill, the beneficiary is the “Telco Co.” account which is a business account that receives many such payments). This way, our transaction graph looks realistic – money flows between individuals and companies, not just random pairings.

In summary, we seed each customer with attributes that drive how the Phase 2 engine behaves for them. This ensures not everyone is doing the same thing at the same time – we’ll have a rich mix of heavy users and light users, big and small transactions, etc. The dataset and workload will thus mimic real production variety.

Incorporating Errors and Edge Cases in Phase 2

As described in the logging section, we will actively simulate errors during the live run as well. Some additional points for Phase 2:
	•	Rate of Failures: We can configure a small probability for each action to fail. For instance, 1% of login attempts fail (wrong password), 0.5% of transactions get declined, etc. These numbers can be tweaked to match observed metrics if available (e.g. if we know that, say, 2% of ATM withdrawals attempts fail due to low balance in real life, we use that). The goal is to cover those code paths and also see how the database logs fill up with error records.
	•	System Outages: Possibly, we could simulate a brief outage of a service (e.g. the ATM network down for 5 minutes) by pausing all ATM operations or making them fail during that window. While not required, this would test the system’s resilience (and create a surge of retries after the outage). This might be beyond scope, but it’s an idea for advanced realism.
	•	Debug/Validation Mode: We will include a debug mode in the generator where it can run a small deterministic scenario (maybe with just 1 or 2 customers) and print all actions, to verify correctness (point 25). This is mostly for our testing of the simulator, but ensures that when we ramp up to thousands of users, we have confidence it behaves correctly. It can also help to verify that with a given random seed, the sequence of events truly repeats exactly.

Reproducibility with Randomization Seed

To allow cross-database comparisons under identical workloads, we will implement a configurable random seed for the simulation. All random number generation in Phase 2 (choosing what action to do next, picking random amounts, scheduling times, selecting which customers are active, etc.) will be driven by a PRNG (pseudo-random number generator) initialized with a seed value. If the seed is the same, the sequence of “random” choices will be the same each run. This means we can run the exact same set of events on different database systems or different test runs for apples-to-apples comparisons.

For example, if seed = 42, our generator might deterministically simulate Customer #1001 doing a login at time X, then a transfer of $Y to account Z, etc., and the entire timeline of events will repeat if we rerun with seed 42. We have to be careful to ensure single-threaded and multi-threaded randomness are handled in a repeatable way (perhaps by partitioning the PRNG per thread or using a thread-safe deterministic approach so timing differences don’t alter the sequence). With some care, we can guarantee that workload replay is possible, which is extremely valuable for benchmarking different databases under the same load.

This seed will be an input parameter. By default it could be random (for variability), but for comparative testing the user can set it to a fixed number.

Monitoring and Adjustability

We will incorporate logging and metrics in the Phase 2 run to observe progress:
	•	A progress meter or stats output will periodically show how many operations have been performed (point 24 suggests a progress bar or similar for generation; in a live loop, we might log TPS, etc., every few seconds). This helps track that the test is running as expected and at what intensity.
	•	CLI Ergonomics: All important simulation parameters (number of users, rate multipliers, error rates, seed, etc.) will be configurable via command-line flags or a config file. We won’t hard-code presets; instead we’ll have clear help text for each parameter (point 23). For instance, --customers=1000 --concurrency=100 --seed=42 --scenario=heavy_weekend etc., with a help screen describing these.
	•	Concurrent Loading in Phase 1: Phase 1 generator can also output progress (e.g. “Generating 10 million transactions… 50% done”). Given unlimited CPU assumption, we can multi-thread the data generation as well. Logging can include timing for each table generation.
	•	Debug/Testing Mode: As mentioned, a debug mode can run a short simulation with verbose logging of each step to validate behavior (point 25). This is mainly for our development, but we mention it as part of completeness.
	•	Stopping Conditions: By default the Phase 2 runs until killed, but we could also include a flag for duration or number of operations (e.g. run 1 hour or 100k transactions then stop) if needed for automated runs. The user explicitly said they will kill it manually, so manual control is assumed.

By implementing all the above, we end up with a high-fidelity simulation of a global bank’s workload:
	•	The database schema is comprehensive and realistic, with customers, accounts, transactions, companies, and audit logs.
	•	The initial state is populated with a believable history.
	•	The live workload behaves like real users across the globe: following daily cycles, performing a realistic mix of reads and writes, including errors, and creating peak loads at expected times.
	•	We have the ability to reproduce tests exactly via seeds, and to adjust parameters easily to test different scenarios.
	•	Logging and auditing is in place to verify activity and potentially measure performance (e.g. we could analyze the audit log afterward to ensure distribution of actions was as expected).

All these measures align with public best practices for realistic data simulation. We drew on known standards like TPC-like benchmarking patterns (mixed read/write OLTP workload) and academic projects like BankSim (which generated synthetic bank transactions based on real data)  for inspiration. BankSim, for example, showed that it’s feasible to approximate real payment networks using statistical patterns from real banks . Similarly, our plan uses real-world patterns (end-of-month spikes, 80/20 activity distribution, etc.) to ensure the dataset and workload are indistinguishable from a real bank’s from a database perspective. By treating the database as the core ledger system and exercising it with realistic operations, this “bank in a box” will provide a robust test of database performance under true-to-life conditions.

With this approach, we’ll be able to push any database to its limits in a way that uncovers how it handles the complexity and scale of global banking workloads, far beyond a simple uniform test. The result will be a highly realistic benchmark environment for evaluating and comparing database systems in the context of core banking.


